{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# **Lightweight Fine-Tuning Project**\\n\",\n",
    "    \"\\n\",\n",
    "    \"* **PEFT technique**: LoRA (Low-Rank Adaptation)\\n\",\n",
    "    \"* **Model**: DistilBERT (distilbert-base-uncased)\\n\",\n",
    "    \"* **Evaluation approach**: Evaluating accuracy and F1 score on validation set using Hugging Face Trainer\\n\",\n",
    "    \"* **Fine-tuning dataset**: SST-2 (Stanford Sentiment Treebank) from GLUE benchmark\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## **Loading and Evaluating a Foundation Model**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from datasets import load_dataset\\n\",\n",
    "    \"from transformers import (\\n\",\n",
    "    \"    AutoModelForSequenceClassification,\\n\",\n",
    "    \"    AutoTokenizer,\\n\",\n",
    "    \"    TrainingArguments,\\n\",\n",
    "    \"    Trainer\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set the device\\n\",\n",
    "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define constants\\n\",\n",
    "    \"MODEL_NAME = \\\"distilbert-base-uncased\\\"  # Smaller model for faster training\\n\",\n",
    "    \"DATASET_NAME = \\\"glue\\\"\\n\",\n",
    "    \"DATASET_CONFIG = \\\"sst2\\\"  # Sentiment analysis task\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the pre-trained model\\n\",\n",
    "    \"model = AutoModelForSequenceClassification.from_pretrained(\\n\",\n",
    "    \"    MODEL_NAME, \\n\",\n",
    "    \"    num_labels=2,  # Binary classification\\n\",\n",
    "    \").to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print model size\\n\",\n",
    "    \"total_params = sum(p.numel() for p in model.parameters())\\n\",\n",
    "    \"print(f\\\"Total parameters in base model: {total_params:,}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load tokenizer\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load dataset \\n\",\n",
    "    \"dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\\n\",\n",
    "    \"print(f\\\"Dataset structure: {dataset}\\\")\\n\",\n",
    "    \"print(f\\\"Example from training set: {dataset['train'][0]}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# To reduce computation, let's use a subset of the data\\n\",\n",
    "    \"train_dataset = dataset[\\\"train\\\"].select(range(1000))  # Use 1000 training examples\\n\",\n",
    "    \"eval_dataset = dataset[\\\"validation\\\"].select(range(200))  # Use 200 validation examples\\n\",\n",
    "    \"print(f\\\"Using {len(train_dataset)} training examples and {len(eval_dataset)} validation examples\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenize the dataset\\n\",\n",
    "    \"def preprocess_function(examples):\\n\",\n",
    "    \"    return tokenizer(\\n\",\n",
    "    \"        examples[\\\"sentence\\\"], \\n\",\n",
    "    \"        truncation=True, \\n\",\n",
    "    \"        padding=\\\"max_length\\\", \\n\",\n",
    "    \"        max_length=128\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply preprocessing\\n\",\n",
    "    \"tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\\n\",\n",
    "    \"tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define custom metrics functions without using sklearn\\n\",\n",
    "    \"def accuracy_score(y_true, y_pred):\\n\",\n",
    "    \"    \\\"\\\"\\\"Calculate accuracy score without using sklearn.\\\"\\\"\\\"\\n\",\n",
    "    \"    if len(y_true) != len(y_pred):\\n\",\n",
    "    \"        raise ValueError(\\\"Input arrays must have the same length\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\\n\",\n",
    "    \"    return correct / len(y_true)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def f1_score(y_true, y_pred):\\n\",\n",
    "    \"    \\\"\\\"\\\"Calculate F1 score for binary classification without using sklearn.\\\"\\\"\\\"\\n\",\n",
    "    \"    if len(y_true) != len(y_pred):\\n\",\n",
    "    \"        raise ValueError(\\\"Input arrays must have the same length\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate true positives, false positives, false negatives\\n\",\n",
    "    \"    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 1)\\n\",\n",
    "    \"    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 0 and pred == 1)\\n\",\n",
    "    \"    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 1 and pred == 0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate precision and recall\\n\",\n",
    "    \"    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\\n\",\n",
    "    \"    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate F1 score\\n\",\n",
    "    \"    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return f1\\n\",\n",
    "    \"\\n\",\n",
    "    \"def compute_metrics(eval_pred):\\n\",\n",
    "    \"    \\\"\\\"\\\"Compute evaluation metrics for the model.\\\"\\\"\\\"\\n\",\n",
    "    \"    logits, labels = eval_pred\\n\",\n",
    "    \"    predictions = np.argmax(logits, axis=-1)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        \\\"accuracy\\\": accuracy_score(labels, predictions),\\n\",\n",
    "    \"        \\\"f1\\\": f1_score(labels, predictions)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up training arguments for evaluation\\n\",\n",
    "    \"base_training_args = TrainingArguments(\\n\",\n",
    "    \"    output_dir=\\\"/tmp/base_eval\\\",\\n\",\n",
    "    \"    per_device_eval_batch_size=16,\\n\",\n",
    "    \"    report_to=\\\"none\\\"  # Disable reporting to avoid unnecessary dependencies\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up trainer for evaluation\\n\",\n",
    "    \"base_trainer = Trainer(\\n\",\n",
    "    \"    model=model,\\n\",\n",
    "    \"    args=base_training_args,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    compute_metrics=compute_metrics,\\n\",\n",
    "    \"    eval_dataset=tokenized_eval_dataset,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate base model\\n\",\n",
    "    \"print(\\\"Evaluating base model...\\\")\\n\",\n",
    "    \"base_model_results = base_trainer.evaluate()\\n\",\n",
    "    \"print(\\\"Base model evaluation results:\\\")\\n\",\n",
    "    \"for key, value in base_model_results.items():\\n\",\n",
    "    \"    print(f\\\"  {key}: {value:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## **Performing Parameter-Efficient Fine-Tuning**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import PEFT libraries\\n\",\n",
    "    \"from peft import get_peft_model, LoraConfig, TaskType\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure PEFT (LoRA)\\n\",\n",
    "    \"peft_config = LoraConfig(\\n\",\n",
    "    \"    task_type=TaskType.SEQ_CLS,  # Sequence classification task\\n\",\n",
    "    \"    inference_mode=False,\\n\",\n",
    "    \"    r=8,                         # Rank of the low-rank decomposition\\n\",\n",
    "    \"    lora_alpha=32,               # Alpha parameter for LoRA scaling\\n\",\n",
    "    \"    lora_dropout=0.1,            # Dropout probability for LoRA layers\\n\",\n",
    "    \"    # Target the attention matrices in DistilBERT\\n\",\n",
    "    \"    target_modules=[\\\"q_lin\\\", \\\"v_lin\\\"]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply PEFT config to the model\\n\",\n",
    "    \"peft_model = get_peft_model(model, peft_config)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print trainable parameters information\\n\",\n",
    "    \"trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\\n\",\n",
    "    \"print(f\\\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%} of total)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print model architecture\\n\",\n",
    "    \"print(peft_model)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training arguments\\n\",\n",
    "    \"training_args = TrainingArguments(\\n\",\n",
    "    \"    output_dir=\\\"/tmp/checkpoints\\\",\\n\",\n",
    "    \"    learning_rate=1e-3,\\n\",\n",
    "    \"    per_device_train_batch_size=8,\\n\",\n",
    "    \"    per_device_eval_batch_size=16,\\n\",\n",
    "    \"    num_train_epochs=3,          # Train for 3 epochs\\n\",\n",
    "    \"    weight_decay=0.01,\\n\",\n",
    "    \"    evaluation_strategy=\\\"epoch\\\",\\n\",\n",
    "    \"    save_strategy=\\\"epoch\\\",\\n\",\n",
    "    \"    load_best_model_at_end=True,\\n\",\n",
    "    \"    report_to=\\\"none\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize trainer\\n\",\n",
    "    \"trainer = Trainer(\\n\",\n",
    "    \"    model=peft_model,\\n\",\n",
    "    \"    args=training_args,\\n\",\n",
    "    \"    train_dataset=tokenized_train_dataset,\\n\",\n",
    "    \"    eval_dataset=tokenized_eval_dataset,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    compute_metrics=compute_metrics,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train the model\\n\",\n",
    "    \"print(\\\"Fine-tuning the model...\\\")\\n\",\n",
    "    \"train_results = trainer.train()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print training results\\n\",\n",
    "    \"print(\\\"Training results:\\\")\\n\",\n",
    "    \"print(f\\\"  Training loss: {train_results.training_loss:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Saving the model\\n\",\n",
    "    \"peft_model_path = \\\"/tmp/peft_model_final\\\"\\n\",\n",
    "    \"trainer.save_model(peft_model_path)\\n\",\n",
    "    \"print(f\\\"Model saved to {peft_model_path}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## **Performing Inference with a PEFT Model**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import PEFT model loading utilities\\n\",\n",
    "    \"from peft import PeftModel, PeftConfig\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load PEFT configuration\\n\",\n",
    "    \"peft_config = PeftConfig.from_pretrained(peft_model_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load base model\\n\",\n",
    "    \"inference_model = AutoModelForSequenceClassification.from_pretrained(\\n\",\n",
    "    \"    peft_config.base_model_name_or_path,\\n\",\n",
    "    \"    num_labels=2\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load PEFT model\\n\",\n",
    "    \"loaded_peft_model = PeftModel.from_pretrained(inference_model, peft_model_path)\\n\",\n",
    "    \"loaded_peft_model.to(device)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Set up trainer for evaluation\\n\",\n",
    "    \"eval_trainer = Trainer(\\n\",\n",
    "    \"    model=loaded_peft_model,\\n\",\n",
    "    \"    args=training_args,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    compute_metrics=compute_metrics,\\n\",\n",
    "    \"    eval_dataset=tokenized_eval_dataset,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate fine-tuned model\\n\",\n",
    "    \"print(\\\"Evaluating fine-tuned model...\\\")\\n\",\n",
    "    \"fine_tuned_results = eval_trainer.evaluate()\\n\",\n",
    "    \"print(\\\"Fine-tuned model evaluation results:\\\")\\n\",\n",
    "    \"for key, value in fine_tuned_results.items():\\n\",\n",
    "    \"    print(f\\\"  {key}: {value:.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare base and fine-tuned model results\\n\",\n",
    "    \"print(\\\"\\\\n=== Model Comparison ===\\\")\\n\",\n",
    "    \"print(\\\"Metric      | Base Model | Fine-tuned Model | Improvement\\\")\\n\",\n",
    "    \"print(\\\"------------|------------|-----------------|------------\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for metric in [\\\"eval_accuracy\\\", \\\"eval_f1\\\"]:\\n\",\n",
    "    \"    base_value = base_model_results.get(metric, 0)\\n\",\n",
    "    \"    fine_tuned_value = fine_tuned_results.get(metric, 0)\\n\",\n",
    "    \"    improvement = fine_tuned_value - base_value\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    metric_name = metric.replace(\\\"eval_\\\", \\\"\\\")\\n\",\n",
    "    \"    print(f\\\"{metric_name:<11} | {base_value:.4f}     | {fine_tuned_value:.4f}          | {improvement:+.4f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Perform sample inference\\n\",\n",
    "    \"sample_examples = [\\n\",\n",
    "    \"    \\\"I absolutely loved this movie, it was fantastic!\\\",\\n\",\n",
    "    \"    \\\"The film was okay, nothing special.\\\",\\n\",\n",
    "    \"    \\\"This is the worst movie I've ever seen.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenize samples\\n\",\n",
    "    \"inputs = tokenizer(sample_examples, return_tensors=\\\"pt\\\", padding=True, truncation=True).to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform inference with the fine-tuned model\\n\",\n",
    "    \"with torch.no_grad():\\n\",\n",
    "    \"    outputs = loaded_peft_model(**inputs)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get predictions\\n\",\n",
    "    \"predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display results\\n\",\n",
    "    \"print(\\\"\\\\nSample Inference:\\\")\\n\",\n",
    "    \"for i, (text, pred) in enumerate(zip(sample_examples, predictions)):\\n\",\n",
    "    \"    sentiment = \\\"Positive\\\" if pred == 1 else \\\"Negative\\\"\\n\",\n",
    "    \"    print(f\\\"Example {i+1}: \\\\\\\"{text}\\\\\\\"\\\")\\n\",\n",
    "    \"    print(f\\\"  Predicted sentiment: {sentiment}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nProject completed successfully!\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
